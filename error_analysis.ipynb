{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1a6f8f5",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "e1a6f8f5",
    "outputId": "2d68d12e-9a7f-43e3-e087-f3fc832fd6be",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import random\n",
    "import allennlp\n",
    "import numpy as np\n",
    "import torch\n",
    "import logging\n",
    "import collections\n",
    "import bert.tokenization as tokenization\n",
    "from bert.modeling_drop import MTMSN\n",
    "from bert.optimization import BERTAdam\n",
    "from drop.drop_utils import DropReader, convert_examples_to_features, get_tensors, get_tensors_list, write_predictions, \\\n",
    "    ClusteredBatcher, FixedOrderBatcher, FeatureLenKey, batch_annotate_candidates, wrapped_get_final_text\n",
    "from drop.drop_metric import DropEmAndF1\n",
    "from bert.run_mtmsn import evaluate\n",
    "from squad.squad_evaluate import f1_score as calculate_f1\n",
    "from squad.squad_utils import _get_best_indexes, get_final_text, _compute_softmax\n",
    "from decimal import Decimal"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e63c3d3",
   "metadata": {},
   "source": [
    "# collection of wrongly predicted answers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbf9dfaa",
   "metadata": {
    "id": "fbf9dfaa",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "logger = logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a52a5607",
   "metadata": {},
   "outputs": [],
   "source": [
    "sign_remap = {0: 0, 1: 1, 2: -1}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0597a143",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 311
    },
    "id": "0597a143",
    "outputId": "4909604c-747f-45f1-f785-f3dc4b2087f0"
   },
   "outputs": [],
   "source": [
    "from bert.modeling import BertConfig\n",
    "bert_config = BertConfig.from_json_file('18/bert_config.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34c6f20f",
   "metadata": {
    "id": "34c6f20f"
   },
   "outputs": [],
   "source": [
    "model = MTMSN(bert_config)\n",
    "tokenizer = tokenization.FullTokenizer(\n",
    "        vocab_file='18/vocab.txt', do_lower_case=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0829a44",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 317
    },
    "id": "b0829a44",
    "outputId": "1494e6ad-b45a-48b9-8b0e-6ac2efac13c9"
   },
   "outputs": [],
   "source": [
    "checkpoint = torch.load('18/checkpoint.pth.tar',map_location=torch.device('cpu'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "362baf56",
   "metadata": {
    "id": "362baf56"
   },
   "outputs": [],
   "source": [
    "from collections import OrderedDict\n",
    "model_cp = []\n",
    "for i in checkpoint['model']:\n",
    "    model_cp.append(('.'.join(i.split('.')[1:]),checkpoint['model'][i]))\n",
    "model_cp = OrderedDict(model_cp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3f9ff51",
   "metadata": {
    "id": "d3f9ff51",
    "outputId": "96637eb7-83a3-4b20-8d3a-7d6cf0c1efd7",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model.load_state_dict(model_cp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3c2c94c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "drop_reader = DropReader(logger=logger)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "328cfce3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "eval_examples = drop_reader._read('data/drop_dataset_test_1.json')\n",
    "\n",
    "eval_features = convert_examples_to_features(\n",
    "    examples=eval_examples,\n",
    "    tokenizer=tokenizer,\n",
    "    max_seq_length=512,\n",
    "    is_train=False,\n",
    "    logger=logger)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51db759b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_result(id):\n",
    "    all_results = []\n",
    "    answering_abilities=['span_extraction', 'addition_subtraction', 'counting', 'negation']\n",
    "    batch_feature = eval_features[id:id+1]\n",
    "    batch = get_tensors(batch_feature, is_train=False)\n",
    "    input_ids, input_mask, segment_ids, number_indices = batch\n",
    "    with torch.no_grad():\n",
    "        output_dict = model(\"normal\", input_ids, segment_ids, input_mask, number_indices)\n",
    "    best_answer_ability = output_dict[\"best_answer_ability\"]\n",
    "    span_start_logits = output_dict[\"span_start_logits\"]\n",
    "    span_end_logits = output_dict[\"span_end_logits\"]\n",
    "    best_span_number = output_dict[\"best_span_number\"]\n",
    "    number_sign_logits = output_dict[\"number_sign_logits\"]\n",
    "    number_mask = output_dict[\"number_mask\"]\n",
    "    encoded_numbers_output = output_dict[\"encoded_numbers_output\"]\n",
    "    passage_output = output_dict[\"passage_output\"]\n",
    "    question_output = output_dict[\"question_output\"]\n",
    "    pooled_output = output_dict[\"pooled_output\"]\n",
    "    best_count_number = output_dict[\"best_count_number\"]\n",
    "    best_negations_for_numbers = output_dict[\"best_negations_for_numbers\"]\n",
    "\n",
    "    batch_result = []\n",
    "    for i, feature in enumerate(batch_feature):\n",
    "        unique_id = int(feature.unique_id)\n",
    "        result = {}\n",
    "        result['unique_id'] = unique_id\n",
    "        result['predicted_ability'] = best_answer_ability[i].detach().cpu().numpy()\n",
    "        result['start_logits'] = span_start_logits[i].detach().cpu().tolist()\n",
    "        result['end_logits'] = span_end_logits[i].detach().cpu().tolist()\n",
    "        result['predicted_spans'] = best_span_number[i].detach().cpu().numpy()\n",
    "        result['number_sign_logits'] = number_sign_logits[i].detach().cpu().numpy()\n",
    "        result['number_mask'] = number_mask[i].detach().cpu().numpy()\n",
    "        result['predicted_count'] = best_count_number[i].detach().cpu().numpy()\n",
    "        result['predicted_negations'] = best_negations_for_numbers[i].detach().cpu().numpy()\n",
    "        batch_result.append(result)\n",
    "    number_indices2, sign_indices, _, sign_scores = \\\n",
    "        batch_annotate_candidates(eval_examples, batch_feature, batch_result, answering_abilities,\n",
    "                                  False, 3, 4)\n",
    "    number_indices2 = torch.tensor(number_indices2, dtype=torch.long)\n",
    "    sign_indices = torch.tensor(sign_indices, dtype=torch.long)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        sign_rerank_logits = model(\"rerank_inference\", input_ids, segment_ids, input_mask, number_indices,\n",
    "                                   number_indices2=number_indices2, sign_indices=sign_indices,\n",
    "                                   encoded_numbers_input=encoded_numbers_output, passage_input=passage_output,\n",
    "                                   question_input=question_output, pooled_input=pooled_output)\n",
    "\n",
    "    for i, result in enumerate(batch_result):\n",
    "        result['number_indices2'] = number_indices2[i].detach().cpu().tolist()\n",
    "        result['sign_indices'] = sign_indices[i].detach().cpu().tolist()\n",
    "        result['sign_rerank_logits'] = sign_rerank_logits[i].detach().cpu().tolist()\n",
    "        result['sign_probs'] = sign_scores[i]\n",
    "    return eval_examples[id], result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "119565b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def out(result, example, feature, n_best_size=20, max_answer_length=30, length_heuristic=0.05, do_lower_case=True, verbose_logging=False):\n",
    "    answering_abilities=['span_extraction', 'addition_subtraction', 'counting', 'negation']\n",
    "    predicted_ability_str = answering_abilities[result['predicted_ability']]\n",
    "    nbest_json, predicted_answers = [], []\n",
    "    _PrelimPrediction = collections.namedtuple(  # pylint: disable=invalid-name\n",
    "    \"PrelimPrediction\",\n",
    "        [\"start_index\", \"end_index\", \"start_logit\", \"end_logit\", \"rerank_logit\", \"heuristic_logit\"])\n",
    "    \n",
    "    if predicted_ability_str == \"addition_subtraction\":\n",
    "        max_prob, best_answer = 0, None\n",
    "        sign_rerank_probs = _compute_softmax(result['sign_rerank_logits'])\n",
    "        for number_indices, sign_indices, rerank_prob, prob in zip(result['number_indices2'], result['sign_indices'], sign_rerank_probs, result['sign_probs']):\n",
    "            pred_answer = sum([sign_remap[sign_index] * example.numbers_in_passage[number_index] for sign_index, number_index in zip(sign_indices, number_indices) if sign_index != -1 and number_index != -1])\n",
    "            pred_answer = str(float(Decimal(pred_answer).quantize(Decimal('0.0000'))))\n",
    "            if rerank_prob*prob > max_prob:\n",
    "                max_prob = rerank_prob*prob\n",
    "                best_answer = pred_answer\n",
    "        assert best_answer is not None\n",
    "        predicted_answers.append(best_answer)\n",
    "        output = {}\n",
    "        output[\"text\"] = str(best_answer)\n",
    "        output[\"type\"] = \"addition_subtraction\"\n",
    "        nbest_json.append(output)\n",
    "    elif predicted_ability_str == \"counting\":\n",
    "        predicted_answers.append(str(result['predicted_count']))\n",
    "        output = {}\n",
    "        output[\"text\"] = str(result['predicted_count'])\n",
    "        output[\"type\"] = \"counting\"\n",
    "        nbest_json.append(output)\n",
    "    elif predicted_ability_str == \"negation\":\n",
    "        index = np.argmax(result['predicted_negations'])\n",
    "        pred_answer = 100 - example.numbers_in_passage[index]\n",
    "        pred_answer = float(Decimal(pred_answer).quantize(Decimal('0.0000')))\n",
    "        predicted_answers.append(str(pred_answer))\n",
    "        output = {}\n",
    "        output[\"text\"] = str(pred_answer)\n",
    "        output[\"type\"] = \"negation\"\n",
    "        nbest_json.append(output)\n",
    "    elif predicted_ability_str == \"span_extraction\":\n",
    "        number_of_spans = result['predicted_spans']\n",
    "        prelim_predictions = []\n",
    "        start_indexes = _get_best_indexes(result['start_logits'], n_best_size)\n",
    "        end_indexes = _get_best_indexes(result['end_logits'], n_best_size)\n",
    "        for start_index in start_indexes:\n",
    "            for end_index in end_indexes:\n",
    "                # We could hypothetically create invalid predictions, e.g., predict\n",
    "                # that the start of the span is in the question. We throw out all\n",
    "                # invalid predictions.\n",
    "                if start_index >= len(feature.tokens):\n",
    "                    continue\n",
    "                if end_index >= len(feature.tokens):\n",
    "                    continue\n",
    "                if start_index not in feature.que_token_to_orig_map and start_index not in feature.doc_token_to_orig_map:\n",
    "                    continue\n",
    "                if end_index not in feature.que_token_to_orig_map and start_index not in feature.doc_token_to_orig_map:\n",
    "                    continue\n",
    "                if end_index < start_index:\n",
    "                    continue\n",
    "                length = end_index - start_index + 1\n",
    "                if length > max_answer_length:\n",
    "                    continue\n",
    "\n",
    "                start_logit = result['start_logits'][start_index]\n",
    "                end_logit = result['end_logits'][end_index]\n",
    "                heuristic_logit = start_logit + end_logit \\\n",
    "                                  - length_heuristic * (end_index - start_index + 1)\n",
    "                prelim_predictions.append(\n",
    "                    _PrelimPrediction(\n",
    "                        start_index=start_index,\n",
    "                        end_index=end_index,\n",
    "                        start_logit=start_logit,\n",
    "                        end_logit=end_logit,\n",
    "                        rerank_logit=0,\n",
    "                        heuristic_logit=heuristic_logit))\n",
    "\n",
    "        prelim_predictions = sorted(prelim_predictions, key=lambda x: (x.heuristic_logit), reverse=True)\n",
    "\n",
    "        _NbestPrediction = collections.namedtuple(  # pylint: disable=invalid-name\n",
    "            \"NbestPrediction\", [\"text\", \"start_logit\", \"end_logit\", \"start_index\", \"end_index\", \"rerank_logit\", \"heuristic_logit\"])\n",
    "\n",
    "        seen_predictions = {}\n",
    "        nbest = []\n",
    "        for i, pred_i in enumerate(prelim_predictions):\n",
    "            if len(nbest) >= n_best_size:\n",
    "                break\n",
    "\n",
    "            final_text = wrapped_get_final_text(example, feature, pred_i.start_index, pred_i.end_index,\n",
    "                                                do_lower_case, verbose_logging, logger)\n",
    "            if final_text in seen_predictions or final_text is None:\n",
    "                continue\n",
    "\n",
    "            seen_predictions[final_text] = True\n",
    "            nbest.append(\n",
    "                _NbestPrediction(\n",
    "                    text=final_text,\n",
    "                    start_logit=pred_i.start_logit,\n",
    "                    end_logit=pred_i.end_logit,\n",
    "                    start_index=pred_i.start_index,\n",
    "                    end_index=pred_i.end_index,\n",
    "                    rerank_logit=pred_i.rerank_logit,\n",
    "                    heuristic_logit=pred_i.heuristic_logit\n",
    "                ))\n",
    "\n",
    "            # filter out redundant candidates\n",
    "            if (i + 1) < len(prelim_predictions):\n",
    "                indexes = []\n",
    "                for j, pred_j in enumerate(prelim_predictions[(i + 1):]):\n",
    "                    filter_text = wrapped_get_final_text(example, feature, pred_j.start_index, pred_j.end_index,\n",
    "                                                         do_lower_case, verbose_logging, logger)\n",
    "                    if filter_text is None:\n",
    "                        indexes.append(i + j + 1)\n",
    "                    else:\n",
    "                        if calculate_f1(final_text, filter_text) > 0:\n",
    "                            indexes.append(i + j + 1)\n",
    "                [prelim_predictions.pop(index - k) for k, index in enumerate(indexes)]\n",
    "\n",
    "        # In very rare edge cases we could have no valid predictions. So we\n",
    "        # just create a nonce prediction in this case to avoid failure.\n",
    "        if not nbest:\n",
    "            nbest.append(\n",
    "                _NbestPrediction(text=\"empty\", start_logit=0.0, end_logit=0.0, start_index=0.0, end_index=0.0,\n",
    "                                 rerank_logit=0., heuristic_logit=0.))\n",
    "\n",
    "        assert len(nbest) >= 1\n",
    "\n",
    "        for i, entry in enumerate(nbest):\n",
    "            if i > number_of_spans:\n",
    "                break\n",
    "            predicted_answers.append(entry.text)\n",
    "            output = {}\n",
    "            output[\"text\"] = entry.text\n",
    "            output[\"type\"] = \"span_extraction\"\n",
    "            nbest_json.append(output)\n",
    "    else:\n",
    "        raise ValueError(f\"Unsupported answer ability: {predicted_ability_str}\")\n",
    "    return nbest_json\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "603aed2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def golden_answer(example):\n",
    "    true = []\n",
    "    true_ans = example.answer_annotations[0]\n",
    "    for k in true_ans.keys():\n",
    "        if k == \"date\":\n",
    "            if true_ans[k].get('day') or true_ans[k].get('month') or true_ans[k].get('year'):\n",
    "                return str(true_ans[k])\n",
    "        elif k == \"spans\":\n",
    "            if len(true_ans[k]) > 0:\n",
    "                return ' '.join(true_ans[k])\n",
    "        elif k == \"number\":\n",
    "            if true_ans[k]:\n",
    "                return true_ans[k]\n",
    "        else:\n",
    "            return None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "248ba3d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def isfloat(value):\n",
    "    try:\n",
    "        float(value)\n",
    "        return True\n",
    "    except ValueError:\n",
    "        return False\n",
    "    \n",
    "def isdigit(value):\n",
    "    try:\n",
    "        int(value.replace(',', ''))\n",
    "        return True\n",
    "    except ValueError:\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80d7758c",
   "metadata": {},
   "outputs": [],
   "source": [
    "wrong = []\n",
    "for i in range(2534, len(eval_features)):\n",
    "    comparison = {}\n",
    "    query, result = get_result(i)\n",
    "    predicted = out(result, example=eval_examples[i], feature=eval_features[i])\n",
    "    pred = predicted[0]['text']\n",
    "    true = golden_answer(eval_examples[i])\n",
    "    if isfloat(pred) and isfloat(true):\n",
    "        if float(pred) == float(true):\n",
    "            continue\n",
    "        else:\n",
    "            comparison['predicted'] = pred\n",
    "            comparison['true'] = true\n",
    "            comparison['passage'] = ' '.join(eval_examples[i].passage_tokens)\n",
    "            comparison['question'] = ' '.join(eval_examples[i].question_tokens)\n",
    "            wrong.append(comparison)  \n",
    "            print(i)\n",
    "            print(comparison)\n",
    "    elif any(str.isdigit(c) for c in pred):\n",
    "        if isfloat(pred) and isfloat(true):\n",
    "            if float(pred) == float(true):\n",
    "                continue\n",
    "            else:\n",
    "                comparison['predicted'] = pred\n",
    "                comparison['true'] = true\n",
    "                comparison['passage'] = ' '.join(eval_examples[i].passage_tokens)\n",
    "                comparison['question'] = ' '.join(eval_examples[i].question_tokens)\n",
    "                wrong.append(comparison)\n",
    "                print(i)\n",
    "                print(comparison)\n",
    "    elif pred == true:\n",
    "        continue\n",
    "    else:\n",
    "        comparison['predicted'] = pred\n",
    "        comparison['true'] = true\n",
    "        comparison['passage'] = ' '.join(eval_examples[i].passage_tokens)\n",
    "        comparison['question'] = ' '.join(eval_examples[i].question_tokens)\n",
    "        wrong.append(comparison)\n",
    "        print(i)\n",
    "        print(comparison)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c330c4b",
   "metadata": {},
   "source": [
    "save current progress, i="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d357cd5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import pickle\n",
    "\n",
    "with open('data/current_progress', 'wb') as fp:\n",
    "    pickle.dump(wrong, fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e8da049",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open ('data/current_progress', 'rb') as fp:\n",
    "    itemlist = pickle.load(fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d65932e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(itemlist)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2c0ca2f",
   "metadata": {},
   "source": [
    "# data aggregation for cross validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4672c05c",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train = {}\n",
    "for i in [0,1,2,3]:\n",
    "    fname = 'fold-' + str(i)\n",
    "    f = open(fname)\n",
    "    data_f = json.load(f)\n",
    "    data_train.update(data_f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f8958bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_test = {}\n",
    "fname = 'fold-4'\n",
    "f = open(fname)\n",
    "data_f = json.load(f)\n",
    "data_test.update(data_f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "525d0a5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('drop_dataset_train_1.json', 'w') as outfile:\n",
    "    json.dump(data_train, outfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3172ae6",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('drop_dataset_test_1.json', 'w') as outf:\n",
    "    json.dump(data_test, outf)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "TPU",
  "colab": {
   "name": "Copy of comp5222.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
